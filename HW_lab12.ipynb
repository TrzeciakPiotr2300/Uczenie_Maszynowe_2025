{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCwfso49b/XUU0O9wxTTFo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrzeciakPiotr2300/Uczenie_Maszynowe_2025/blob/main/HW_lab12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Step one, set up the environment*** ###"
      ],
      "metadata": {
        "id": "iI8paihAyoFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d_qz-ngvyWid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e9b342-7ce2-4de5-f878-2a9ea9f1f3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install stable versions to avoid numpy-related bugs\n",
        "!pip install --upgrade --quiet gensim==4.3.0 scikit-learn==1.4.1.post1 numpy==1.24.4 matplotlib==3.7.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim.downloader as api\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm"
      ],
      "metadata": {
        "id": "uFe7_pXAyefY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "8893f9e0-f2b9-43a2-eb0e-f163f927e2b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1695602334>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsi\u001b[0m  \u001b[0;31m# gamma function utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained GloVe embeddings (100-dimensional)\n",
        "embedding_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Helper function to fetch vector\n",
        "def get_vector(word):\n",
        "    return embedding_model[word] if word in embedding_model else None\n",
        "\n",
        "# Helper function to fetch multiple vectors\n",
        "def get_vectors(words):\n",
        "    return np.array([get_vector(word) for word in words if get_vector(word) is not None])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "UtLmo-2qzQf8",
        "outputId": "1959abb3-9ba7-4e4b-cbfb-31e1592e44a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'api' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1041344566>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained GloVe embeddings (100-dimensional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-wiki-gigaword-100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Helper function to fetch vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Define semantic words classes**"
      ],
      "metadata": {
        "id": "yrbkZTZB0dzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 10 semantic classes, each with 5 word pairs\n",
        "semantic_classes = {\n",
        "    \"gender\": [\n",
        "        (\"king\", \"queen\"),\n",
        "        (\"man\", \"woman\"),\n",
        "        (\"boy\", \"girl\"),\n",
        "        (\"husband\", \"wife\"),\n",
        "        (\"actor\", \"actress\")\n",
        "    ],\n",
        "    \"past_tense\": [\n",
        "        (\"run\", \"ran\"),\n",
        "        (\"go\", \"went\"),\n",
        "        (\"write\", \"wrote\"),\n",
        "        (\"eat\", \"ate\"),\n",
        "        (\"speak\", \"spoke\")\n",
        "    ],\n",
        "    \"plural\": [\n",
        "        (\"dog\", \"dogs\"),\n",
        "        (\"car\", \"cars\"),\n",
        "        (\"child\", \"children\"),\n",
        "        (\"mouse\", \"mice\"),\n",
        "        (\"foot\", \"feet\")\n",
        "    ],\n",
        "    \"comparative\": [\n",
        "        (\"fast\", \"faster\"),\n",
        "        (\"small\", \"smaller\"),\n",
        "        (\"happy\", \"happier\"),\n",
        "        (\"strong\", \"stronger\"),\n",
        "        (\"bright\", \"brighter\")\n",
        "    ],\n",
        "    \"superlative\": [\n",
        "        (\"fast\", \"fastest\"),\n",
        "        (\"small\", \"smallest\"),\n",
        "        (\"happy\", \"happiest\"),\n",
        "        (\"strong\", \"strongest\"),\n",
        "        (\"bright\", \"brightest\")\n",
        "    ],\n",
        "    \"country_capital\": [\n",
        "        (\"france\", \"paris\"),\n",
        "        (\"italy\", \"rome\"),\n",
        "        (\"germany\", \"berlin\"),\n",
        "        (\"japan\", \"tokyo\"),\n",
        "        (\"egypt\", \"cairo\")\n",
        "    ],\n",
        "    \"currency\": [\n",
        "        (\"usa\", \"dollar\"),\n",
        "        (\"japan\", \"yen\"),\n",
        "        (\"uk\", \"pound\"),\n",
        "        (\"france\", \"euro\"),\n",
        "        (\"china\", \"yuan\")\n",
        "    ],\n",
        "    \"tool_user\": [\n",
        "        (\"pen\", \"writer\"),\n",
        "        (\"hammer\", \"carpenter\"),\n",
        "        (\"brush\", \"painter\"),\n",
        "        (\"scalpel\", \"surgeon\"),\n",
        "        (\"camera\", \"photographer\")\n",
        "    ],\n",
        "    \"job_object\": [\n",
        "        (\"chef\", \"pan\"),\n",
        "        (\"driver\", \"car\"),\n",
        "        (\"farmer\", \"tractor\"),\n",
        "        (\"pianist\", \"piano\"),\n",
        "        (\"teacher\", \"book\")\n",
        "    ],\n",
        "    \"animal_sound\": [\n",
        "        (\"dog\", \"bark\"),\n",
        "        (\"cat\", \"meow\"),\n",
        "        (\"cow\", \"moo\"),\n",
        "        (\"duck\", \"quack\"),\n",
        "        (\"lion\", \"roar\")\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "MAIxjtxY0K9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Now we gather all unique words and their vectors:\n",
        "# Flatten to get all unique words\n",
        "all_words = set()\n",
        "for pairs in semantic_classes.values():\n",
        "    for w1, w2 in pairs:\n",
        "        all_words.add(w1)\n",
        "        all_words.add(w2)\n",
        "\n",
        "# Get vectors only for words present in embedding model\n",
        "word_vectors = {word: get_vector(word) for word in all_words if get_vector(word) is not None}\n",
        "\n",
        "# Final word list (with embeddings)\n",
        "words_with_vectors = list(word_vectors.keys())\n",
        "print(f\"Total unique words with embeddings: {len(words_with_vectors)}\")\n"
      ],
      "metadata": {
        "id": "hNph8QFx0kJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Now we will do PCA analysis to visualise wodr embeddings in a 2D space**##"
      ],
      "metadata": {
        "id": "MYXuiQeM03VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "ddu_mHk_0vE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_words_2d(word_list, vectors, title):\n",
        "    pca = PCA(n_components=2)\n",
        "    X = np.array([vectors[word] for word in word_list])\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    #plt.xlim(-4.1, 7.0)\n",
        "    #plt.ylim(-3.5, 3.5)\n",
        "    for i, word in enumerate(word_list):\n",
        "        plt.scatter(X_pca[i, 0], X_pca[i, 1])\n",
        "        plt.text(X_pca[i, 0] + 0.02, X_pca[i, 1] + 0.02, word, fontsize=9)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "KozENub80_wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Global PCA\n",
        "plot_words_2d(words_with_vectors, word_vectors, \"Global PCA: All Words\")"
      ],
      "metadata": {
        "id": "Wh-7qEWt1FTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Global PCA, but one class at a time &&& in a global space:\n",
        "for class_name, pairs in semantic_classes.items():\n",
        "    class_words = [w for pair in pairs for w in pair if w in word_vectors]\n",
        "    plot_words_2d(class_words, word_vectors, f\"Global PCA - Class: {class_name}\")\n"
      ],
      "metadata": {
        "id": "VkjB5NUg1HZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local PCA — each class separatelly, so PCA for 10 words in each class.\n",
        "def plot_local_pca(class_name, pairs):\n",
        "    class_words = [w for pair in pairs for w in pair if w in word_vectors]\n",
        "    class_vectors = {word: word_vectors[word] for word in class_words}\n",
        "    plot_words_2d(class_words, class_vectors, f\"Local PCA - Class: {class_name}\")\n",
        "\n",
        "for class_name, pairs in semantic_classes.items():\n",
        "    plot_local_pca(class_name, pairs)\n"
      ],
      "metadata": {
        "id": "kVWxYHUH1euM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**And now we will do some embeddings, meaning calculate expressions such as: embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") ~= \"queen\"**"
      ],
      "metadata": {
        "id": "qP7Lkhow3I8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "X7-7EPrz2ZKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_words(vec, vectors_dict, topn=5):\n",
        "    all_words = list(vectors_dict.keys())\n",
        "    all_vecs = np.array([vectors_dict[w] for w in all_words])\n",
        "\n",
        "    sims = cosine_similarity([vec], all_vecs)[0]\n",
        "    sorted_indices = np.argsort(sims)[::-1]\n",
        "\n",
        "    results = []\n",
        "    for idx in sorted_indices[:topn]:\n",
        "        results.append((all_words[idx], sims[idx]))\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Q8aAA71E3WsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculations for every class:\n",
        "for class_name, pairs in semantic_classes.items():\n",
        "    base_pair = pairs[0]  # (A, B)\n",
        "    alt_pair = pairs[1]   # (C, D), weźmiemy C\n",
        "\n",
        "    A, B = base_pair\n",
        "    C, _ = alt_pair\n",
        "\n",
        "    if A in word_vectors and B in word_vectors and C in word_vectors:\n",
        "        result_vec = word_vectors[B] - word_vectors[A] + word_vectors[C]\n",
        "        top_matches = find_closest_words(result_vec, word_vectors)\n",
        "\n",
        "        print(f\"\\n=== Class: {class_name} ===\")\n",
        "        print(f\"Operation: {B} - {A} + {C}\")\n",
        "        for word, score in top_matches:\n",
        "            print(f\"{word:<15} similarity: {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\nSkipping class '{class_name}' due to missing word(s)\")\n"
      ],
      "metadata": {
        "id": "h0ebDgyw3Yh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Conclusion for second part of discussion (for PCA plots, we will make another 10 plots)**"
      ],
      "metadata": {
        "id": "lQYQ2UwX47Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform global PCA for all words with vectors\n",
        "global_pca = PCA(n_components=2)\n",
        "words = list(word_vectors.keys())\n",
        "vectors = np.array([word_vectors[word] for word in words])\n",
        "reduced_embeddings = global_pca.fit_transform(vectors)\n",
        "\n",
        "# Create a dictionary: word -> global PCA coordinates\n",
        "global_pca_dict = {word: reduced_embeddings[i] for i, word in enumerate(words)}\n",
        "\n",
        "# Plot overlay charts for each class (global vs local PCA)\n",
        "for class_name, pairs in semantic_classes.items():\n",
        "    words_in_class = [word for pair in pairs for word in pair if word in word_vectors]\n",
        "\n",
        "    # Skip class if any word is missing (to avoid KeyError)\n",
        "    if not all(word in global_pca_dict for word in words_in_class):\n",
        "        print(f\"Skipping {class_name}: missing words in global PCA\")\n",
        "        continue\n",
        "\n",
        "    # Get global PCA coordinates for words in this class\n",
        "    global_coords = np.array([global_pca_dict[word] for word in words_in_class])\n",
        "\n",
        "    # Compute local PCA for just the words in this class\n",
        "    local_vectors = np.array([word_vectors[word] for word in words_in_class])\n",
        "    pca_local = PCA(n_components=2)\n",
        "    local_coords = pca_local.fit_transform(local_vectors)\n",
        "\n",
        "    # Plotting the overlay chart\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    for i, word in enumerate(words_in_class):\n",
        "        # Global: blue circles\n",
        "        plt.scatter(global_coords[i, 0], global_coords[i, 1], color='blue', marker='o', label='Global PCA' if i == 0 else \"\")\n",
        "        plt.text(global_coords[i, 0]+0.01, global_coords[i, 1]+0.01, word, fontsize=9, color='blue')\n",
        "\n",
        "        # Local: red crosses\n",
        "        plt.scatter(local_coords[i, 0], local_coords[i, 1], color='red', marker='x', label='Local PCA' if i == 0 else \"\")\n",
        "        plt.text(local_coords[i, 0]+0.01, local_coords[i, 1]+0.01, word, fontsize=9, color='red')\n",
        "\n",
        "    plt.title(f\"PCA Global vs Local — {class_name}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "V6cDpldC3caf",
        "outputId": "8d697254-48c8-4acb-9c8d-676afb6033fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reduced_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-346497089>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Globalne embeddingi po PCA były zapisane w reduced_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Potrzebujemy słownika: słowo -> współrzędne PCA (globalne)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mglobal_pca_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduced_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Rysujemy overlay wykresy dla każdej klasy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-346497089>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Globalne embeddingi po PCA były zapisane w reduced_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Potrzebujemy słownika: słowo -> współrzędne PCA (globalne)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mglobal_pca_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduced_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Rysujemy overlay wykresy dla każdej klasy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reduced_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKAiK-eW6OMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}