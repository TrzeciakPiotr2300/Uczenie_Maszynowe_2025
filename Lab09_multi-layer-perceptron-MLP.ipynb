{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrzeciakPiotr2300/Uczenie_Maszynowe_2025/blob/main/Lab09_multi-layer-perceptron-MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Multi Layer Perceptron Notation\n",
        "---------------------------------\n",
        "\n",
        "In this workshop we will be classifying 28 by 28 images into 10 classes. Thus, a four layer perceptron (our classificator) we will work further with can be defined as\n",
        "$\\hat f:\\mathbb{R}^{28\\cdot 28} \\rightarrow \\mathbb{R}^{10}$ defined as\n",
        "\n",
        "$$\\hat f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right) =  W_4 \\left[ W_3 \\left[ W_2 \\left[ W_1 x  + b_1 \\right]_+  + b_2 \\right]_+  + b_3 \\right]_+ + b_4,$$\n",
        "\n",
        "where matrices $W_1, \\ldots, W_4$ are tensors of order two (matrices) with matching dimensions and bias terms $b_1, \\ldots, b_4$ are tensors of order one (vectors) of matching dimensions, and we are using ReLU activation.\n",
        "\n",
        "Note, that there is no nonlinear activation after the last layer in our neural network. **There is an implicit softmax applied while cross entropy loss is calculated by `torch.nn.functional`.**\n"
      ],
      "metadata": {
        "id": "wqJVU0gp9rws"
      },
      "id": "wqJVU0gp9rws"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading MNIST Dataset to Play with It\n",
        "--------------------"
      ],
      "metadata": {
        "id": "s6RwV0RN_Kc9"
      },
      "id": "s6RwV0RN_Kc9"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=None)"
      ],
      "metadata": {
        "id": "vjAlmExA_Twh",
        "outputId": "bd709b89-ee66-40ad-86e6-3c5dd3d42ab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vjAlmExA_Twh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 339kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.27MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "gcgep60g_Xs6",
        "outputId": "b1451990-d01c-4752-de25-e49db3f8a68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "id": "gcgep60g_Xs6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
      ],
      "metadata": {
        "id": "Ar2Rzq7UAcjC",
        "outputId": "68f8f0f2-69c4-44d1-c770-2eac2b0e08f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ar2Rzq7UAcjC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.1306604762738429), np.float64(0.30810780385646264))"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "---------------\n",
        "\n",
        "Why do we need to normalize the data, and not feed the NN with the 0-255 integers?"
      ],
      "metadata": {
        "id": "-waIu3OQXGx0"
      },
      "id": "-waIu3OQXGx0"
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=2048,\n",
        "                                          shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=1,\n",
        "                                         shuffle=False)"
      ],
      "metadata": {
        "id": "01dayZiEAk3C"
      },
      "id": "01dayZiEAk3C",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Data\n"
      ],
      "metadata": {
        "id": "dfqt0kUECnZB"
      },
      "id": "dfqt0kUECnZB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labels (Ground Truth Outputs)"
      ],
      "metadata": {
        "id": "fxGIwBKGCtFa"
      },
      "id": "fxGIwBKGCtFa"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ],
      "metadata": {
        "id": "bzu3lJ2lCwFq",
        "outputId": "a7616a56-a698-4f01-bdd0-dae88e7f5b86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bzu3lJ2lCwFq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch labels : tensor([4, 4, 2,  ..., 8, 6, 1])\n",
            "1 -th batch labels : tensor([2, 8, 8,  ..., 3, 7, 6])\n",
            "2 -th batch labels : tensor([0, 0, 0,  ..., 3, 5, 2])\n",
            "3 -th batch labels : tensor([7, 5, 9,  ..., 8, 3, 8])\n",
            "4 -th batch labels : tensor([5, 5, 0,  ..., 1, 8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch."
      ],
      "metadata": {
        "id": "PebIPPfpC05T"
      },
      "id": "PebIPPfpC05T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs"
      ],
      "metadata": {
        "id": "NLmKJ1FbC2qJ"
      },
      "id": "NLmKJ1FbC2qJ"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i==0:\n",
        "            print(i, \"-th batch inputs :\", batch_inputs)"
      ],
      "metadata": {
        "id": "2jcRrNw_DQui",
        "outputId": "2d9a5d12-72cd-4a38-a318-c36eb410d137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2jcRrNw_DQui",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by `ToTensor()` transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten` layer or by using `squeeze()` on a tensor."
      ],
      "metadata": {
        "id": "YTcC3qH2DZOK"
      },
      "id": "YTcC3qH2DZOK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Definition\n",
        "-----------------\n",
        "\n",
        "Your job now is to take the (fully functional) definition of the MLP structure and get rid off the Sequential layer.\n"
      ],
      "metadata": {
        "id": "ZX09I7rUDkei"
      },
      "id": "ZX09I7rUDkei"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(1 * 28 * 28, 1024)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(1024, 2048)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(2048, 256)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "TSoP6W2UECwb"
      },
      "id": "TSoP6W2UECwb",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "----------------------"
      ],
      "metadata": {
        "id": "fE5rN8UTFFw7"
      },
      "id": "fE5rN8UTFFw7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on {device}\")\n",
        "\n",
        "net = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001.\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)  #explicitly moving the data to the target device\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "                                ####You can experiment - comment this line and check, that the loss DOE"
      ],
      "metadata": {
        "id": "55Sda8Y5FIOh",
        "outputId": "747a015b-075e-4a7b-bf4e-733b5dee649a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "55Sda8Y5FIOh",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cuda\n",
            "epoch: 0 batch: 0 current batch loss: 2.3051631450653076\n",
            "epoch: 0 batch: 1 current batch loss: 2.0698280334472656\n",
            "epoch: 0 batch: 2 current batch loss: 1.612332820892334\n",
            "epoch: 0 batch: 3 current batch loss: 1.1740645170211792\n",
            "epoch: 0 batch: 4 current batch loss: 0.9418269395828247\n",
            "epoch: 0 batch: 5 current batch loss: 1.190300703048706\n",
            "epoch: 0 batch: 6 current batch loss: 1.1286660432815552\n",
            "epoch: 0 batch: 7 current batch loss: 0.9571342468261719\n",
            "epoch: 0 batch: 8 current batch loss: 0.7238796353340149\n",
            "epoch: 0 batch: 9 current batch loss: 0.6118448972702026\n",
            "epoch: 0 batch: 10 current batch loss: 0.6451218724250793\n",
            "epoch: 0 batch: 11 current batch loss: 0.557117223739624\n",
            "epoch: 0 batch: 12 current batch loss: 0.5865345001220703\n",
            "epoch: 0 batch: 13 current batch loss: 0.5366986989974976\n",
            "epoch: 0 batch: 14 current batch loss: 0.4726025462150574\n",
            "epoch: 0 batch: 15 current batch loss: 0.4472435414791107\n",
            "epoch: 0 batch: 16 current batch loss: 0.462693452835083\n",
            "epoch: 0 batch: 17 current batch loss: 0.4245925545692444\n",
            "epoch: 0 batch: 18 current batch loss: 0.42585816979408264\n",
            "epoch: 0 batch: 19 current batch loss: 0.3888839781284332\n",
            "epoch: 0 batch: 20 current batch loss: 0.38070324063301086\n",
            "epoch: 0 batch: 21 current batch loss: 0.36230331659317017\n",
            "epoch: 0 batch: 22 current batch loss: 0.342117041349411\n",
            "epoch: 0 batch: 23 current batch loss: 0.3487149477005005\n",
            "epoch: 0 batch: 24 current batch loss: 0.2878001630306244\n",
            "epoch: 0 batch: 25 current batch loss: 0.30674195289611816\n",
            "epoch: 0 batch: 26 current batch loss: 0.27970612049102783\n",
            "epoch: 0 batch: 27 current batch loss: 0.3248178958892822\n",
            "epoch: 0 batch: 28 current batch loss: 0.2931481897830963\n",
            "epoch: 0 batch: 29 current batch loss: 0.3032473623752594\n",
            "epoch: 1 batch: 0 current batch loss: 0.26000481843948364\n",
            "epoch: 1 batch: 1 current batch loss: 0.240330308675766\n",
            "epoch: 1 batch: 2 current batch loss: 0.24884819984436035\n",
            "epoch: 1 batch: 3 current batch loss: 0.24384814500808716\n",
            "epoch: 1 batch: 4 current batch loss: 0.24514338374137878\n",
            "epoch: 1 batch: 5 current batch loss: 0.23681475222110748\n",
            "epoch: 1 batch: 6 current batch loss: 0.23640033602714539\n",
            "epoch: 1 batch: 7 current batch loss: 0.22984693944454193\n",
            "epoch: 1 batch: 8 current batch loss: 0.22006770968437195\n",
            "epoch: 1 batch: 9 current batch loss: 0.24034731090068817\n",
            "epoch: 1 batch: 10 current batch loss: 0.20372702181339264\n",
            "epoch: 1 batch: 11 current batch loss: 0.20895524322986603\n",
            "epoch: 1 batch: 12 current batch loss: 0.1863032579421997\n",
            "epoch: 1 batch: 13 current batch loss: 0.20589201152324677\n",
            "epoch: 1 batch: 14 current batch loss: 0.21134965121746063\n",
            "epoch: 1 batch: 15 current batch loss: 0.18296073377132416\n",
            "epoch: 1 batch: 16 current batch loss: 0.185917466878891\n",
            "epoch: 1 batch: 17 current batch loss: 0.18648761510849\n",
            "epoch: 1 batch: 18 current batch loss: 0.19368833303451538\n",
            "epoch: 1 batch: 19 current batch loss: 0.18589439988136292\n",
            "epoch: 1 batch: 20 current batch loss: 0.18811333179473877\n",
            "epoch: 1 batch: 21 current batch loss: 0.19844062626361847\n",
            "epoch: 1 batch: 22 current batch loss: 0.18388429284095764\n",
            "epoch: 1 batch: 23 current batch loss: 0.17585544288158417\n",
            "epoch: 1 batch: 24 current batch loss: 0.1778057962656021\n",
            "epoch: 1 batch: 25 current batch loss: 0.18347908556461334\n",
            "epoch: 1 batch: 26 current batch loss: 0.148883655667305\n",
            "epoch: 1 batch: 27 current batch loss: 0.17506463825702667\n",
            "epoch: 1 batch: 28 current batch loss: 0.16650395095348358\n",
            "epoch: 1 batch: 29 current batch loss: 0.18601299822330475\n",
            "epoch: 2 batch: 0 current batch loss: 0.12148038297891617\n",
            "epoch: 2 batch: 1 current batch loss: 0.12529726326465607\n",
            "epoch: 2 batch: 2 current batch loss: 0.14582253992557526\n",
            "epoch: 2 batch: 3 current batch loss: 0.14919675886631012\n",
            "epoch: 2 batch: 4 current batch loss: 0.12886971235275269\n",
            "epoch: 2 batch: 5 current batch loss: 0.14986877143383026\n",
            "epoch: 2 batch: 6 current batch loss: 0.1508508324623108\n",
            "epoch: 2 batch: 7 current batch loss: 0.11638225615024567\n",
            "epoch: 2 batch: 8 current batch loss: 0.14389526844024658\n",
            "epoch: 2 batch: 9 current batch loss: 0.15152138471603394\n",
            "epoch: 2 batch: 10 current batch loss: 0.14551430940628052\n",
            "epoch: 2 batch: 11 current batch loss: 0.12565714120864868\n",
            "epoch: 2 batch: 12 current batch loss: 0.13292409479618073\n",
            "epoch: 2 batch: 13 current batch loss: 0.14327016472816467\n",
            "epoch: 2 batch: 14 current batch loss: 0.1489507257938385\n",
            "epoch: 2 batch: 15 current batch loss: 0.14859753847122192\n",
            "epoch: 2 batch: 16 current batch loss: 0.11172075569629669\n",
            "epoch: 2 batch: 17 current batch loss: 0.12413402646780014\n",
            "epoch: 2 batch: 18 current batch loss: 0.1576593816280365\n",
            "epoch: 2 batch: 19 current batch loss: 0.128413125872612\n",
            "epoch: 2 batch: 20 current batch loss: 0.1319359540939331\n",
            "epoch: 2 batch: 21 current batch loss: 0.11759690195322037\n",
            "epoch: 2 batch: 22 current batch loss: 0.1210300624370575\n",
            "epoch: 2 batch: 23 current batch loss: 0.1279820203781128\n",
            "epoch: 2 batch: 24 current batch loss: 0.12996922433376312\n",
            "epoch: 2 batch: 25 current batch loss: 0.11212550848722458\n",
            "epoch: 2 batch: 26 current batch loss: 0.12220144271850586\n",
            "epoch: 2 batch: 27 current batch loss: 0.13170620799064636\n",
            "epoch: 2 batch: 28 current batch loss: 0.10710185021162033\n",
            "epoch: 2 batch: 29 current batch loss: 0.08532655984163284\n",
            "epoch: 3 batch: 0 current batch loss: 0.1060672476887703\n",
            "epoch: 3 batch: 1 current batch loss: 0.09076429903507233\n",
            "epoch: 3 batch: 2 current batch loss: 0.10438628494739532\n",
            "epoch: 3 batch: 3 current batch loss: 0.11738943308591843\n",
            "epoch: 3 batch: 4 current batch loss: 0.09346745908260345\n",
            "epoch: 3 batch: 5 current batch loss: 0.1019434705376625\n",
            "epoch: 3 batch: 6 current batch loss: 0.09268863499164581\n",
            "epoch: 3 batch: 7 current batch loss: 0.10037146508693695\n",
            "epoch: 3 batch: 8 current batch loss: 0.07726398855447769\n",
            "epoch: 3 batch: 9 current batch loss: 0.08987576514482498\n",
            "epoch: 3 batch: 10 current batch loss: 0.1207522302865982\n",
            "epoch: 3 batch: 11 current batch loss: 0.09882869571447372\n",
            "epoch: 3 batch: 12 current batch loss: 0.08467661589384079\n",
            "epoch: 3 batch: 13 current batch loss: 0.09893207252025604\n",
            "epoch: 3 batch: 14 current batch loss: 0.09589748829603195\n",
            "epoch: 3 batch: 15 current batch loss: 0.08781494945287704\n",
            "epoch: 3 batch: 16 current batch loss: 0.10879024863243103\n",
            "epoch: 3 batch: 17 current batch loss: 0.09015214443206787\n",
            "epoch: 3 batch: 18 current batch loss: 0.08102685958147049\n",
            "epoch: 3 batch: 19 current batch loss: 0.10180409997701645\n",
            "epoch: 3 batch: 20 current batch loss: 0.0922851413488388\n",
            "epoch: 3 batch: 21 current batch loss: 0.0782320499420166\n",
            "epoch: 3 batch: 22 current batch loss: 0.07979333400726318\n",
            "epoch: 3 batch: 23 current batch loss: 0.08925312012434006\n",
            "epoch: 3 batch: 24 current batch loss: 0.08867589384317398\n",
            "epoch: 3 batch: 25 current batch loss: 0.08394444733858109\n",
            "epoch: 3 batch: 26 current batch loss: 0.08291765302419662\n",
            "epoch: 3 batch: 27 current batch loss: 0.07824500650167465\n",
            "epoch: 3 batch: 28 current batch loss: 0.08582895249128342\n",
            "epoch: 3 batch: 29 current batch loss: 0.09485961496829987\n",
            "epoch: 4 batch: 0 current batch loss: 0.08096194267272949\n",
            "epoch: 4 batch: 1 current batch loss: 0.09686153382062912\n",
            "epoch: 4 batch: 2 current batch loss: 0.06279966980218887\n",
            "epoch: 4 batch: 3 current batch loss: 0.07534974813461304\n",
            "epoch: 4 batch: 4 current batch loss: 0.08559738099575043\n",
            "epoch: 4 batch: 5 current batch loss: 0.07040835916996002\n",
            "epoch: 4 batch: 6 current batch loss: 0.0688168928027153\n",
            "epoch: 4 batch: 7 current batch loss: 0.065888412296772\n",
            "epoch: 4 batch: 8 current batch loss: 0.07854820787906647\n",
            "epoch: 4 batch: 9 current batch loss: 0.06906437128782272\n",
            "epoch: 4 batch: 10 current batch loss: 0.07362061738967896\n",
            "epoch: 4 batch: 11 current batch loss: 0.07555462419986725\n",
            "epoch: 4 batch: 12 current batch loss: 0.07816480845212936\n",
            "epoch: 4 batch: 13 current batch loss: 0.08045285195112228\n",
            "epoch: 4 batch: 14 current batch loss: 0.07680200040340424\n",
            "epoch: 4 batch: 15 current batch loss: 0.064705990254879\n",
            "epoch: 4 batch: 16 current batch loss: 0.06499014794826508\n",
            "epoch: 4 batch: 17 current batch loss: 0.08499348908662796\n",
            "epoch: 4 batch: 18 current batch loss: 0.06126686558127403\n",
            "epoch: 4 batch: 19 current batch loss: 0.06150456517934799\n",
            "epoch: 4 batch: 20 current batch loss: 0.08288460969924927\n",
            "epoch: 4 batch: 21 current batch loss: 0.07154354453086853\n",
            "epoch: 4 batch: 22 current batch loss: 0.07305588573217392\n",
            "epoch: 4 batch: 23 current batch loss: 0.06880350410938263\n",
            "epoch: 4 batch: 24 current batch loss: 0.07126758992671967\n",
            "epoch: 4 batch: 25 current batch loss: 0.06642870604991913\n",
            "epoch: 4 batch: 26 current batch loss: 0.07132329791784286\n",
            "epoch: 4 batch: 27 current batch loss: 0.08038564026355743\n",
            "epoch: 4 batch: 28 current batch loss: 0.07536581158638\n",
            "epoch: 4 batch: 29 current batch loss: 0.08168937265872955\n",
            "epoch: 5 batch: 0 current batch loss: 0.060894232243299484\n",
            "epoch: 5 batch: 1 current batch loss: 0.05836471542716026\n",
            "epoch: 5 batch: 2 current batch loss: 0.05664729326963425\n",
            "epoch: 5 batch: 3 current batch loss: 0.056300222873687744\n",
            "epoch: 5 batch: 4 current batch loss: 0.06590896844863892\n",
            "epoch: 5 batch: 5 current batch loss: 0.0494120828807354\n",
            "epoch: 5 batch: 6 current batch loss: 0.06187310442328453\n",
            "epoch: 5 batch: 7 current batch loss: 0.05975247919559479\n",
            "epoch: 5 batch: 8 current batch loss: 0.06818674504756927\n",
            "epoch: 5 batch: 9 current batch loss: 0.05546458065509796\n",
            "epoch: 5 batch: 10 current batch loss: 0.06264326721429825\n",
            "epoch: 5 batch: 11 current batch loss: 0.06252246350049973\n",
            "epoch: 5 batch: 12 current batch loss: 0.047900665551424026\n",
            "epoch: 5 batch: 13 current batch loss: 0.05449966341257095\n",
            "epoch: 5 batch: 14 current batch loss: 0.05580812692642212\n",
            "epoch: 5 batch: 15 current batch loss: 0.0622742585837841\n",
            "epoch: 5 batch: 16 current batch loss: 0.05894308537244797\n",
            "epoch: 5 batch: 17 current batch loss: 0.06855669617652893\n",
            "epoch: 5 batch: 18 current batch loss: 0.0677277222275734\n",
            "epoch: 5 batch: 19 current batch loss: 0.055069100111722946\n",
            "epoch: 5 batch: 20 current batch loss: 0.05772863328456879\n",
            "epoch: 5 batch: 21 current batch loss: 0.05284116789698601\n",
            "epoch: 5 batch: 22 current batch loss: 0.053517017513513565\n",
            "epoch: 5 batch: 23 current batch loss: 0.05256028100848198\n",
            "epoch: 5 batch: 24 current batch loss: 0.055789899080991745\n",
            "epoch: 5 batch: 25 current batch loss: 0.06911982595920563\n",
            "epoch: 5 batch: 26 current batch loss: 0.05380403250455856\n",
            "epoch: 5 batch: 27 current batch loss: 0.05077388137578964\n",
            "epoch: 5 batch: 28 current batch loss: 0.05869854986667633\n",
            "epoch: 5 batch: 29 current batch loss: 0.0610298290848732\n",
            "epoch: 6 batch: 0 current batch loss: 0.04319381341338158\n",
            "epoch: 6 batch: 1 current batch loss: 0.048299916088581085\n",
            "epoch: 6 batch: 2 current batch loss: 0.04971367120742798\n",
            "epoch: 6 batch: 3 current batch loss: 0.041468411684036255\n",
            "epoch: 6 batch: 4 current batch loss: 0.03675199672579765\n",
            "epoch: 6 batch: 5 current batch loss: 0.042996425181627274\n",
            "epoch: 6 batch: 6 current batch loss: 0.06003594771027565\n",
            "epoch: 6 batch: 7 current batch loss: 0.05207362398505211\n",
            "epoch: 6 batch: 8 current batch loss: 0.041389863938093185\n",
            "epoch: 6 batch: 9 current batch loss: 0.042280662804841995\n",
            "epoch: 6 batch: 10 current batch loss: 0.043364427983760834\n",
            "epoch: 6 batch: 11 current batch loss: 0.04138004407286644\n",
            "epoch: 6 batch: 12 current batch loss: 0.04272377863526344\n",
            "epoch: 6 batch: 13 current batch loss: 0.04811276122927666\n",
            "epoch: 6 batch: 14 current batch loss: 0.04977278783917427\n",
            "epoch: 6 batch: 15 current batch loss: 0.04733347147703171\n",
            "epoch: 6 batch: 16 current batch loss: 0.05866120755672455\n",
            "epoch: 6 batch: 17 current batch loss: 0.055009517818689346\n",
            "epoch: 6 batch: 18 current batch loss: 0.04812237620353699\n",
            "epoch: 6 batch: 19 current batch loss: 0.05522819235920906\n",
            "epoch: 6 batch: 20 current batch loss: 0.04504869878292084\n",
            "epoch: 6 batch: 21 current batch loss: 0.04476621001958847\n",
            "epoch: 6 batch: 22 current batch loss: 0.0414598174393177\n",
            "epoch: 6 batch: 23 current batch loss: 0.05203039571642876\n",
            "epoch: 6 batch: 24 current batch loss: 0.05015223100781441\n",
            "epoch: 6 batch: 25 current batch loss: 0.05459019914269447\n",
            "epoch: 6 batch: 26 current batch loss: 0.044444236904382706\n",
            "epoch: 6 batch: 27 current batch loss: 0.04249073192477226\n",
            "epoch: 6 batch: 28 current batch loss: 0.03842174634337425\n",
            "epoch: 6 batch: 29 current batch loss: 0.054010603576898575\n",
            "epoch: 7 batch: 0 current batch loss: 0.03208285942673683\n",
            "epoch: 7 batch: 1 current batch loss: 0.03529052063822746\n",
            "epoch: 7 batch: 2 current batch loss: 0.033293772488832474\n",
            "epoch: 7 batch: 3 current batch loss: 0.038781505078077316\n",
            "epoch: 7 batch: 4 current batch loss: 0.04277988150715828\n",
            "epoch: 7 batch: 5 current batch loss: 0.0338527150452137\n",
            "epoch: 7 batch: 6 current batch loss: 0.0463462732732296\n",
            "epoch: 7 batch: 7 current batch loss: 0.04719752445816994\n",
            "epoch: 7 batch: 8 current batch loss: 0.04271046817302704\n",
            "epoch: 7 batch: 9 current batch loss: 0.03736387565732002\n",
            "epoch: 7 batch: 10 current batch loss: 0.0458780862390995\n",
            "epoch: 7 batch: 11 current batch loss: 0.03641388192772865\n",
            "epoch: 7 batch: 12 current batch loss: 0.034386977553367615\n",
            "epoch: 7 batch: 13 current batch loss: 0.0479913167655468\n",
            "epoch: 7 batch: 14 current batch loss: 0.03425676375627518\n",
            "epoch: 7 batch: 15 current batch loss: 0.0371948704123497\n",
            "epoch: 7 batch: 16 current batch loss: 0.032814398407936096\n",
            "epoch: 7 batch: 17 current batch loss: 0.033975742757320404\n",
            "epoch: 7 batch: 18 current batch loss: 0.026910284534096718\n",
            "epoch: 7 batch: 19 current batch loss: 0.03673192486166954\n",
            "epoch: 7 batch: 20 current batch loss: 0.0361732542514801\n",
            "epoch: 7 batch: 21 current batch loss: 0.035454802215099335\n",
            "epoch: 7 batch: 22 current batch loss: 0.042515527456998825\n",
            "epoch: 7 batch: 23 current batch loss: 0.035200491547584534\n",
            "epoch: 7 batch: 24 current batch loss: 0.041440267115831375\n",
            "epoch: 7 batch: 25 current batch loss: 0.044900041073560715\n",
            "epoch: 7 batch: 26 current batch loss: 0.04851306602358818\n",
            "epoch: 7 batch: 27 current batch loss: 0.04304054006934166\n",
            "epoch: 7 batch: 28 current batch loss: 0.05243499577045441\n",
            "epoch: 7 batch: 29 current batch loss: 0.04144484922289848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "----------------------\n",
        "\n",
        "Correct the code below so it works."
      ],
      "metadata": {
        "id": "sTe6ViIsHUbV"
      },
      "id": "sTe6ViIsHUbV"
    },
    {
      "cell_type": "code",
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint.to(device))                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ],
      "metadata": {
        "id": "jcPek-rrHYMi",
        "outputId": "3fa40ed7-6ada-47d8-ce2b-ebeef20cb5df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jcPek-rrHYMi",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment - *Do Androids Dream of Electric Sheep?***\n",
        "\n",
        "-------------------------------------  \n",
        "\n",
        "\"Do Androids Dream of Electric Sheep?\" – the famous title of Philip K. Dick’s novel – raises a fascinating question: if artificial intelligence could dream, what would it see?  \n",
        "\n",
        "In this assignment, we explore a phenomenon known as **neural network dreams**, where instead of optimizing a neural network's weights, we **optimize the input itself** to achieve a desired classification outcome. Given a fully trained MNIST classification network, your goal is to manipulate its inputs so that it confidently predicts each digit from 0 to 9, starting from pure noise.  \n",
        "\n",
        "## **Tasks Description**  \n",
        "\n",
        "During this class we designed and trained a **MNIST classification neural network**, which takes a **batch of grayscale images** of size **$28 \\times 28$** as input and outputs a probability distribution over the 10 digit classes (0–9). However, instead of using real MNIST images, you will **treat the input batch itself as a set of trainable parameters** and optimize it so that the network classifies each image as a specific digit.  \n",
        "\n",
        "1. Your first task is to generate **a batch of 10 images**, where each image is\n",
        "   classified as one of the digits **0, 1, 2, ..., 9**, starting from an initial batch of ten random Gaussian noise images.  \n",
        "\n",
        "   Discuss the following question: do the generated images resemble real MNIST digits? Why or why not?  \n",
        "\n",
        "2. Discuss, how you would approach a second task of\n",
        "   generating an image that   \n",
        "   bares similarity to two or more digits simultaneously. **Implement your idea to see the results.**\n",
        "\n",
        "3. Third task: repeat the previous tasks with an additional L2 penalty on noise within the images. Experiment with adding `lambda_l2 * dreamed_input_batch.pow(2).mean()` loss term, with `lambda_l2` being the penalty cooefficient within an exponential progression, say from 0.001 to 10.0. Are the new digits recognized correctly? How does the penalty impact the digit quality? Explain.\n",
        "\n",
        "### **Optimization Process for Task 1**  \n",
        "\n",
        "1. Start with a **batch of 10 random Gaussian noise images** as the initial input and $(0, 1, 2, \\ldots, 9)$ as the expected output batch of target digits.  \n",
        "2. Define the objective: maximize the neural network's confidence for the corresponding target digit for each image in the batch.  \n",
        "3. Use **gradient descent** to modify the pixels in each image, making the network classify each one as the assigned digit.  \n",
        "4. Repeat until the network assigns suffieciently high confidence to each image’s target class.  \n",
        "\n",
        "### **Implementation Details**  \n",
        "\n",
        "- The neural network weights **must remain frozen** during optimization. You are modifying only the input images.  \n",
        "- The loss function should be the **cross-entropy loss** between the predicted probabilities and the desired class labels (plus an optional weighted L2 penalty regularizing the images in task 3).\n",
        "\n",
        "\n",
        "## **Points to Note**  \n",
        "\n",
        "1. **Visualize** the optimization process: Save images of the generated inputs at different steps and plot the classification confidence evolution over iterations.  \n",
        "3. **Document your findings** and explain the behavior you observe.  \n",
        "\n",
        "## **Task & Deliverables**  \n",
        "\n",
        "- A **Colab notebook** containing solutions for both tasks:\n",
        "  - The full implementation.\n",
        "  - Visualizations of the generated batch of images.\n",
        "  - A written explanation of your observations.\n",
        "- **Bonus:** If you create an **animation** showing the evolution of the input images during optimization, it will be considered a strong enhancement to your submission.\n",
        "  - You can generate an animation programmatically (e.g., using Matplotlib or OpenCV).\n",
        "  - Or, save image frames and use external tools to create a video.\n",
        "  - Provide a **link** to any video files in the README.\n",
        "- Upload your notebook and results to your **GitHub repository** for the course.\n",
        "- In the **README**, include a **link** to the notebook.\n",
        "- In the notebook, include **“Open in Colab”** badge so it can be launched directly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJ5c7P6ChB3e"
      },
      "id": "ZJ5c7P6ChB3e"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use the same device as your model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Generating images on {device}\")\n",
        "\n",
        "# Define target labels 0–9 and move to the same device\n",
        "targets = torch.arange(10, device=device)\n",
        "\n",
        "# Create 10 images initialized with Gaussian noise and move to device\n",
        "images = torch.randn((10, 1, 28, 28), device=device, requires_grad=True)\n",
        "\n",
        "# Use optimizer to update the images (not model parameters)\n",
        "optimizer = torch.optim.Adam([images], lr=0.1)\n",
        "\n",
        "# Make sure model is in evaluation mode\n",
        "net.eval()\n",
        "\n",
        "# Optimization loop\n",
        "for step in range(2000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = -F.cross_entropy(outputs, targets)  # maximize correct class\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Clamp and move final images to CPU for visualization\n",
        "final_images = images.detach().clamp(0, 1).cpu()\n"
      ],
      "metadata": {
        "id": "DAayS6xUtKuL",
        "outputId": "2d0ec319-1de9-495e-9dd0-e097d26d8a20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DAayS6xUtKuL",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating images on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(final_images[i][0], cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(str(i))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S3OmoYa5tLVe",
        "outputId": "6c1e58f7-31dc-413c-ae63-46839f2ef66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "id": "S3OmoYa5tLVe",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x200 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iFXD21oktx_y"
      },
      "id": "iFXD21oktx_y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}