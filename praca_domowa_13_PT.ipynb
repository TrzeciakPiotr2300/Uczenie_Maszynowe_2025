{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3303eaff",
   "metadata": {},
   "source": [
    "# Praca domowa 13\n",
    "Zadanie polega na lokalizacji jednego nietypowego obiektu w obrazie przy użyciu mechanizmu self-attention.\n",
    "Model ma za zadanie podać współrzędne środka tego obiektu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae1041",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jaksta1/Uczenie_Maszynowe_2025/blob/main/Jakub_Kownacki_praca_domowa_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "id": "2296b6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T18:00:38.676700Z",
     "start_time": "2025-06-30T18:00:37.834219Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Ustawienie seeda\n",
    "def custom_set_seed(seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def custom___init__(self, dim, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)                  # (max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))  # (dim/2,)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # (max_len, dim/2)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # (max_len, dim/2)\n",
    "        self.pe = pe.unsqueeze(0)  # (1, max_len, dim)\n",
    "\n",
    "    def custom_forward(self, x):  # x: (B, N, D)\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)  # (B, N, D)\n",
    "\n",
    "# Self-Attention with Dropout\n",
    "class SelfAttention(nn.Module):\n",
    "    def custom___init__(self, d_detector_model, k=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(d_detector_model, k)     # (D) -> (k)\n",
    "        self.k_proj = nn.Linear(d_detector_model, k)     # (D) -> (k)\n",
    "        self.v_proj = nn.Linear(d_detector_model, d_detector_model)  # (D) -> (D)\n",
    "        self.scale = math.sqrt(k)  # scalar (float)\n",
    "        self.dropout = nn.Dropout(dropout)  # (B, N, D) -> (B, N, D)\n",
    "\n",
    "    def custom_forward(self, x):  # x: (B, N, D)\n",
    "        Q = self.q_proj(x)  # (B, N, k)\n",
    "        K = self.k_proj(x)  # (B, N, k)\n",
    "        V = self.v_proj(x)  # (B, N, D)\n",
    "        attn = torch.bmm(Q, K.transpose(1, 2)) / self.scale  # (B, N, N)\n",
    "        attn = F.softmax(attn, dim=-1)  # (B, N, N)\n",
    "        attn = self.dropout(attn)  # (B, N, N)\n",
    "        self.attn_weights = attn  # (B, N, N)\n",
    "        return torch.bmm(attn, V)  # (B, N, D)\n",
    "\n",
    "# Główny detector_model\n",
    "class OddShapeDetector(nn.Module):\n",
    "    def custom___init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, stride=4, padding=2),  # (B, 1, 64, 64) -> (B, 16, 16, 16)\n",
    "            nn.ReLU(),                                # (B, 16, 16, 16)\n",
    "            nn.Conv2d(16, 16, 3, padding=1),           # (B, 16, 16, 16)\n",
    "            nn.ReLU()                                  # (B, 16, 16, 16)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(2)  # (B, 16, 16, 16) -> (B, 16, 256)\n",
    "        self.transpose = lambda x: x.transpose(1, 2)  # (B, 16, 256) -> (B, 256, 16)\n",
    "        self.pos_enc = PositionalEncoding(dim=16, max_len=256)\n",
    "\n",
    "        # Normalization and attention\n",
    "        self.norm1 = nn.LayerNorm(16)  # (B, 256, 16)\n",
    "        self.attn = SelfAttention(d_detector_model=16, k=8, dropout=0.1)  # (B, 256, 16) -> (B, 256, 16)\n",
    "        self.norm2 = nn.LayerNorm(16)  # (B, 256, 16)\n",
    "\n",
    "        # Heads with dropout\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(16, 16),     # (B, 256, 16) -> (B, 256, 16)\n",
    "            nn.ReLU(),             # (B, 256, 16)\n",
    "            nn.Dropout(0.1),       # (B, 256, 16)\n",
    "            nn.Linear(16, 1)       # (B, 256, 16) -> (B, 256, 1)\n",
    "        )\n",
    "        self.offset_head = nn.Sequential(\n",
    "            nn.Linear(16, 16),     # (B, 256, 16) -> (B, 256, 16)\n",
    "            nn.ReLU(),             # (B, 256, 16)\n",
    "            nn.Dropout(0.1),       # (B, 256, 16)\n",
    "            nn.Linear(16, 2)       # (B, 256, 16) -> (B, 256, 2)\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"grid_centers\", self._make_centers())  # (256, 2)\n",
    "\n",
    "    def custom__make_centers(self):\n",
    "        coords = torch.linspace(2, 62, 16)  # (16,)\n",
    "        grid_y, grid_x = torch.meshgrid(coords, coords, indexing='ij')  # (16, 16), (16, 16)\n",
    "        centers = torch.stack([grid_x, grid_y], dim=-1).reshape(-1, 2)  # (16, 16, 2) -> (256, 2)\n",
    "        return centers\n",
    "\n",
    "    def custom_forward(self, x):\n",
    "        B = x.size(0)\n",
    "        feats = self.cnn(x)                     # (B, 1, 64, 64) -> (B, 16, 16, 16)\n",
    "        feats = self.flatten(feats)             # (B, 16, 16, 16) -> (B, 16, 256)\n",
    "        feats = self.transpose(feats)           # (B, 16, 256) -> (B, 256, 16)\n",
    "        feats = self.pos_enc(feats)             # (B, 256, 16)\n",
    "        feats = self.norm1(feats)               # (B, 256, 16)\n",
    "        attended = self.attn(feats)             # (B, 256, 16)\n",
    "        attended = self.norm2(attended)         # (B, 256, 16)\n",
    "\n",
    "        logits = self.cls_head(attended).squeeze(-1)  # (B, 256, 1) -> (B, 256)\n",
    "        probs = F.softmax(logits, dim=-1)             # (B, 256)\n",
    "        offsets = self.offset_head(attended)          # (B, 256, 2)\n",
    "\n",
    "        pred = (probs.unsqueeze(-1) * (self.grid_centers + offsets)).sum(dim=1)  # (B, 256, 1) * (B, 256, 2) -> (B, 256, 2) -> sum -> (B, 2)\n",
    "        return pred, probs  # (B, 2), (B, 256)\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "598411e8",
   "metadata": {
    "id": "sYcMkO2yhVBh"
   },
   "source": [
    "_____________________________________\n",
    "#1. Architektura sieci\n",
    "_____________________________________\n",
    "1. **Wejście**  \n",
    "   Sieć przyjmuje obrazy w odcieniach szarości o wymiarach 64×64 pikseli, reprezentowane jako tensor o kształcie **(B, 1, 64, 64)**, gdzie B to liczność partii (batch size). Każdy piksel jest znormalizowany do zakresu [0, 1], co stabilizuje proces uczenia i pozwala uniknąć problemów z różnymi skalami wartości.\n",
    "\n",
    "2. **Bloki konwolucyjne (CNN)**  \n",
    "   Pierwsza warstwa konwolucyjna transformuje wejście z 1 kanału do 16 kanałów, stosując filtr 5×5, krok (stride) równy 4 i dopełnienie (padding) 2. Dzięki temu rozdzielczość przestrzenna obrazu redukuje się z 64×64 do 16×16, a filtr o większym polu recepcyjnym umożliwia wychwycenie większych struktur. Po każdej konwolucji stosowana jest funkcja aktywacji ReLU, wprowadzająca nieliniowość. Druga warstwa, z filtrami 3×3 i paddingiem 1, utrzymuje rozmiar 16×16, ale umożliwia głębsze kodowanie cech lokalnych poprzez dodatkową konwolucję w obrębie już zredukowanej siatki.\n",
    "\n",
    "3. **Transformacja do sekwencji cech**  \n",
    "   Po przetworzeniu przez bloki CNN uzyskujemy tensor **(B, 16, 16, 16)**. Następnie stosujemy spłaszczanie wzdłuż dwóch ostatnich wymiarów (Flatten), przekształcając go w **(B, 16, 256)**. Kolejna transpozycja zamienia osie, tak by otrzymać **(B, 256, 16)** – 256 elementów sekwencji, z których każdy jest 16-wymiarowym wektorem cech odpowiadającym jednemu „patchowi” 16×16 w oryginalnym obrazie.\n",
    "\n",
    "4. **Kodowanie pozycyjne**  \n",
    "   Aby sieć wiedziała, która pozycja w sekwencji odpowiada której części obrazu, do wektorów cech dodawane jest kodowanie pozycyjne o wymiarze **(1, 256, 16)**. Dla każdej z 256 pozycji generowane są unikalne wzorce sinusoidalne i kosinusoidalne, o skalowaniu bazującym na wykładniku logarytmu z 10000. Dzięki temu sieć zyskuje dostęp do informacji o bezwzględnej pozycji każdego wektora w siatce.\n",
    "\n",
    "5. **Warstwa self-attention**  \n",
    "   Model tworzy trzy projekcje każdego wektora cech: zapytań Q (16→8), kluczy K (16→8) i wartości V (16→16). Następnie oblicza macierz podobieństwa jako iloczyn Q·Kᵀ podzielony przez √8, a softmax normalizuje wyniki wzdłuż drugiego wymiaru, uzyskując wagi atencji. Dodatkowo stosowany jest dropout (0.1), by ograniczyć nadmierne dopasowanie. Na wejściu i wyjściu tej warstwy stosowana jest warstwa normalizująca LayerNorm, co poprawia stabilność uczenia.\n",
    "\n",
    "6. **Głowy predykcyjne**  \n",
    "   Każdy z 256 przetworzonych wektorów cech trafia teraz do dwóch niezależnych MLP:  \n",
    "   - **cls_head**: MLP o architekturze 16→16 (ReLU)→dropout(0.1)→1, zwracające logit, który po softmaxie staje się wagą mówiącą, z jakim prawdopodobieństwem dany patch zawiera odstający kształt.  \n",
    "   - **offset_head**: MLP 16→16 (ReLU)→dropout(0.1)→2, przewidujące przesunięcie (dx, dy) względem środka danego patcha. To przesunięcie pozwala doprecyzować położenie kształtu wewnątrz każdego subregionu.\n",
    "\n",
    "7. **Siatka centrów (grid_centers)**  \n",
    "   W buforze `grid_centers` przechowywana jest stała macierz o kształcie **(256, 2)**, zawierająca współrzędne środków każdego patcha w oryginalnym obrazie. Są one równomiernie rozmieszczone od 2 do 62 pikseli zarówno w osi x, jak i y, co wynika z faktu, że krok konwolucji wynosi 4 px. Dzięki temu znane jest wyjściowe odniesienie, do którego dodawane są przewidywane przesunięcia z `offset_head`.\n",
    "\n",
    "8. **Łączenie wyników w predykcję**  \n",
    "   Dla każdego z 256 patchy obliczamy jego przewidywane centrum, dodając wektor przesunięcia do odpowiadającego punktu z `grid_centers`. Następnie wykonywane jest *miękkie uśrednienie ważone*: każde takie przewidywane położenie mnożone jest przez odpowiadające mu prawdopodobieństwo z `cls_head`, a wyniki sumowane po wszystkich pozycjach. Otrzymujemy w ten sposób jedną, precyzyjną parę współrzędnych (x, y) wskazującą na położenie odstającego kształtu w całym obrazie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72235016",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "IMAGE = 64\n",
    "SHAPES = (\"circle\", \"square\", \"triangle\")\n",
    "\n",
    "def custom_draw_shape(drawer, shape_type, center_x, center_y, radius):\n",
    "    if shape_type == \"circle\":\n",
    "        drawer.ellipse([center_x - radius, center_y - radius,\n",
    "                        center_x + radius, center_y + radius], fill=\"black\")\n",
    "    elif shape_type == \"square\":\n",
    "        drawer.rectangle([center_x - radius, center_y - radius,\n",
    "                          center_x + radius, center_y + radius], fill=\"black\")\n",
    "    else:  # triangle\n",
    "        drawer.polygon([\n",
    "            (center_x, center_y - radius),\n",
    "            (center_x - radius, center_y + radius),\n",
    "            (center_x + radius, center_y + radius)\n",
    "        ], fill=\"black\")\n",
    "\n",
    "class OddXYDataset(Dataset):\n",
    "    def custom___init__(self,\n",
    "                 num_samples,\n",
    "                 same_shape_count_range=(3, 6),\n",
    "                 shape_radius_range=(4, 10)):\n",
    "        self.num_samples = num_samples\n",
    "        self.same_shape_count_range = same_shape_count_range\n",
    "        self.radius_min, self.radius_max = shape_radius_range\n",
    "\n",
    "    def custom___len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def custom___getitem__(self, idx):\n",
    "        base_shape = random.choice(SHAPES)\n",
    "        odd_shape = random.choice([s for s in SHAPES if s != base_shape])\n",
    "\n",
    "        img = Image.new(\"L\", (IMAGE, IMAGE), \"white\")\n",
    "        drawer = ImageDraw.Draw(img)\n",
    "\n",
    "        for _ in range(random.randint(*self.same_shape_count_range)):\n",
    "            radius = random.randint(self.radius_min, self.radius_max)\n",
    "            cx = random.randint(radius, IMAGE - radius - 1)\n",
    "            cy = random.randint(radius, IMAGE - radius - 1)\n",
    "            draw_shape(drawer, base_shape, cx, cy, radius)\n",
    "\n",
    "        radius = random.randint(self.radius_min, self.radius_max)\n",
    "        cx = random.randint(radius, IMAGE - radius - 1)\n",
    "        cy = random.randint(radius, IMAGE - radius - 1)\n",
    "        draw_shape(drawer, odd_shape, cx, cy, radius)\n",
    "\n",
    "        img_tensor = torch.tensor(np.array(img), dtype=torch.float32).unsqueeze(0) / 255.\n",
    "        label_tensor = torch.tensor([float(cx), float(cy)], dtype=torch.float32)\n",
    "        return img_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "def custom_compute_rmse(preds, targets):\n",
    "    return torch.sqrt(((preds - targets) ** 2).sum(dim=1)).mean().item()\n",
    "\n",
    "# Training loop\n",
    "def custom_fit_detector_model(detector_model, fit_loader, epochs=100, base_lr=0.001, patience=10):\n",
    "    detector_model.to(device)\n",
    "    optimizer = torch.optim.AdamW(detector_model.parameters(), lr=base_lr, weight_decay=1e-4)\n",
    "\n",
    "    # Scheduler zmniejszający lr dziesięciokrotnie jeśli przez 4 epoki nie nastąpiła poprawa\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=4, min_lr=0.000001\n",
    "        )\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    best_rmse = float(\"inf\")\n",
    "    fit_hist = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        set_seed(42+100*epoch)\n",
    "        detector_model.fit()\n",
    "        for xb, yb in fit_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred, _ = detector_model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(detector_model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "        # RMSE obliczany na końcu epoki\n",
    "        fit_rmse = evaluate_rmse(detector_model, fit_loader)\n",
    "        fit_hist.append(fit_rmse)\n",
    "        print(f\"Epoch {epoch:02d} | LR: {optimizer.param_groups[0]['lr']:.6f} | Train RMSE: {fit_rmse:.2f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if fit_rmse < best_rmse:\n",
    "            best_rmse = fit_rmse\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'detector_model': detector_model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                }, \"checkpoint.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stop: RMSE stagnated.\")\n",
    "                break\n",
    "        # Scheduler krok\n",
    "        scheduler.step(fit_rmse)\n",
    "\n",
    "    return fit_hist\n",
    "\n",
    "def custom_evaluate_rmse(detector_model, loader):\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out, _ = detector_model(xb)\n",
    "            preds.append(out)\n",
    "            targets.append(yb)\n",
    "    return compute_rmse(torch.cat(preds), torch.cat(targets))\n",
    "\n",
    "# Plotting\n",
    "def custom_plot_rmse(fit_rmse):\n",
    "    plt.plot(fit_rmse, label='Train RMSE')\n",
    "    plt.axhline(5.0, color='red', linestyle='--', label='Target RMSE = 5.0')\n",
    "    plt.axhline(3.0, color='blue', linestyle='--', label='Target RMSE = 3.0')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(1)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    fit_set = OddXYDataset(num_samples=25000)\n",
    "\n",
    "    fit_loader = DataLoader(fit_set, batch_size=128, shuffle=True)\n",
    "\n",
    "    detector_model = OddShapeDetector()\n",
    "    fit_rmse = fit_detector_model(\n",
    "    detector_model,\n",
    "    fit_loader,\n",
    "    epochs=300,\n",
    "    base_lr=0.01\n",
    "    )\n",
    "    plot_rmse(fit_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f281c",
   "metadata": {
    "id": "ieRlgv5YQjFF"
   },
   "source": [
    "________________________________________________\n",
    "#2. Opis treningu sieci\n",
    "------------------------------------------------\n",
    "Model OddShapeDetector trenowany jest na sztucznie wygenerowanym zbiorze danych OddXYDataset, zawierającym 25 000 próbek. Dane te są ładowane do modelu w partiach po 128 przykładów z zamieszaną kolejnością, co wspomaga uogólnianie.\n",
    "\n",
    "Proces optymalizacji wykorzystuje algorytm AdamW, który oprócz standardowej aktualizacji wag uwzględnia również regularyzację L2 (tzw. weight decay). Początkowy współczynnik uczenia (learning rate) wynosi 0.01, a jego minimalna wartość to 0.000001. Dodatkowo zastosowano scheduler ReduceLROnPlateau, który zmniejsza współczynnik uczenia dziesięciokrotnie, jeśli przez cztery kolejne epoki nie zostanie zaobserwowana poprawa metryki RMSE.\n",
    "\n",
    "W każdej epoce treningowej dane są przepuszczane przez model, obliczane jest MSE, a następnie wagi są aktualizowane na podstawie gradientów. Dla zwiększenia stabilności uczenia zastosowano gradient clipping – ograniczenie długości wektora gradientu do wartości 1.0.\n",
    "\n",
    "Po zakończeniu każdej epoki model oceniany jest na pełnym zbiorze treningowym poprzez obliczenie wartości RMSE. Jeśli RMSE ulegnie poprawie, stan modelu zostaje zapisany do pliku. Jeśli przez 10 kolejnych epok nie nastąpi żadna poprawa, proces uczenia zostaje zakończony wcześniej (early stopping).\n",
    "\n",
    "Dodatkowo kod zapewnia spójność losowości między epokami, zmieniając ziarno generatora pseudolosowego przy każdej iteracji. Dzięki temu każdy trening jest deterministyczny i możliwy do odtworzenia.\n",
    "\n",
    "Na zakończenie, przebieg RMSE w czasie jest wizualizowany na wykresie, z zaznaczonymi liniami odniesienia dla docelowych wartości RMSE (5.0 i 3.0), co pozwala szybko ocenić skuteczność uczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import torch\n",
    "\n",
    "\n",
    "def custom_visualize_attention_triplet(detector_model, dataset, idx):\n",
    "    \"\"\"\n",
    "    Dla danego indeksu:\n",
    "    - pokazuje obraz wejściowy z predykcją i GT\n",
    "    - pokazuje attention heatmap (standardowa)\n",
    "    - pokazuje attention heatmap (log skala)\n",
    "\n",
    "    Obrazy prezentowane w 1 wierszu.\n",
    "    \"\"\"\n",
    "    detector_model.eval()\n",
    "    img, label = dataset[idx]\n",
    "    img_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred, _ = detector_model(img_tensor)\n",
    "        attn = detector_model.attn.attn_weights.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    img_np = img.squeeze().cpu().numpy()\n",
    "    label_np = label.numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # === 1. Obraz wejściowy z GT i predykcja oraz współrzędne GT ===\n",
    "    axes[0].imshow(img_np, cmap='gray')\n",
    "    axes[0].scatter(*label_np, c='green', s=50, label='GT')\n",
    "    axes[0].scatter(*pred_np, c='red', marker='x', s=50, label='Pred')\n",
    "    # Dodanie tekstu z współrzędnymi GT\n",
    "    x_gt, y_gt = label_np\n",
    "    axes[0].text(\n",
    "        x_gt, y_gt - 5,  # nieco nad punktem\n",
    "        f\"GT: ({x_gt:.1f}, {y_gt:.1f})\",\n",
    "        color='green', fontsize=10, fontweight='bold',\n",
    "        ha='center', va='bottom'\n",
    "    )\n",
    "    axes[0].set_title(f\"Sample {idx} | RMSE: {np.linalg.norm(label_np - pred_np):.2f}\")\n",
    "    axes[0].axis('off')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # === 2. Standardowa heatmapa ===\n",
    "    im1 = axes[1].imshow(attn, cmap='hot')\n",
    "    axes[1].set_title(\"Attention Matrix\")\n",
    "    axes[1].set_xlabel(\"Key index\")\n",
    "    axes[1].set_ylabel(\"Query index\")\n",
    "    fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "    # === 3. LogNorm heatmapa ===\n",
    "    im2 = axes[2].imshow(attn, cmap='inferno', norm=LogNorm(vmin=attn.min()+1e-6, vmax=attn.max()))\n",
    "    axes[2].set_title(\"LogNorm Attention\")\n",
    "    axes[2].set_xlabel(\"Key index\")\n",
    "    axes[2].set_ylabel(\"Query index\")\n",
    "    fig.colorbar(im2, ax=axes[2], label=\"log-scaled weight\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === Uruchomienie ===\n",
    "set_seed(234325352)\n",
    "random.seed(234325352)\n",
    "evaluate_set = OddXYDataset(num_samples=25000)\n",
    "indices_to_visualize = random.sample(range(len(evaluate_set)), 20)\n",
    "for idx in indices_to_visualize:\n",
    "    visualize_attention_triplet(detector_model, evaluate_set, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Funkcja: Mapowanie key_index na środek siatki ===\n",
    "def custom_map_center(idx):\n",
    "    row, col = divmod(idx, 16)\n",
    "    x = 2 + 4 * col\n",
    "    y = 2 + 4 * row\n",
    "    return x, y\n",
    "def custom_visualize_attention_auto_keys(detector_model, dataset, idx, top_k=3):\n",
    "    \"\"\"\n",
    "    Wizualizuje:\n",
    "    - obraz wejściowy z GT, predykcją i automatycznie wybranymi top-K key punktami (na podstawie attention)\n",
    "    - standardową heatmapę attention\n",
    "    - log-skala heatmapy attention\n",
    "    \"\"\"\n",
    "    detector_model.eval()\n",
    "    img, label = dataset[idx]\n",
    "    img_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred, _ = detector_model(img_tensor)\n",
    "        attn = detector_model.attn.attn_weights.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    img_np = img.squeeze().cpu().numpy()\n",
    "    label_np = label.numpy()\n",
    "\n",
    "    # === Wyciągnięcie top K key_indexów ===\n",
    "    attn_sum = attn.sum(axis=0)  # sumujemy kolumny (wpływ poszczególnych keyów)\n",
    "    top_keys = np.argsort(attn_sum)[-top_k:][::-1]  # top K indeksów (największe wpływy)\n",
    "\n",
    "    # === Tworzenie wykresu ===\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # --- 1. Obraz z GT, predykcją i top attention punktami ---\n",
    "    axes[0].imshow(img_np, cmap='gray')\n",
    "    axes[0].scatter(*label_np, c='green', s=50, label='GT')\n",
    "    axes[0].scatter(*pred_np, c='red', marker='x', s=50, label='Pred')\n",
    "\n",
    "    # Dodanie top attention key punktów\n",
    "    for key_idx in top_keys:\n",
    "        x, y = map_center(key_idx)\n",
    "        axes[0].scatter(x, y, c='blue', s=40, marker='o', label='Top Attention')\n",
    "        axes[0].text(x, y + 3, f\"{key_idx}\", color='blue', fontsize=8, ha='center')\n",
    "\n",
    "    axes[0].set_title(f\"Sample {idx} | RMSE: {np.linalg.norm(label_np - pred_np):.2f}\")\n",
    "    axes[0].axis('off')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # --- 2. Standardowa heatmapa attention ---\n",
    "    im1 = axes[1].imshow(attn, cmap='hot')\n",
    "    axes[1].set_title(\"Attention Matrix\")\n",
    "    axes[1].set_xlabel(\"Key index\")\n",
    "    axes[1].set_ylabel(\"Query index\")\n",
    "    fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "    # --- 3. Heatmapa z log skala ---\n",
    "    im2 = axes[2].imshow(attn, cmap='inferno', norm=LogNorm(vmin=attn.min() + 1e-6, vmax=attn.max()))\n",
    "    axes[2].set_title(\"LogNorm Attention\")\n",
    "    axes[2].set_xlabel(\"Key index\")\n",
    "    axes[2].set_ylabel(\"Query index\")\n",
    "    fig.colorbar(im2, ax=axes[2], label=\"log-scaled weight\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Opcjonalnie: wypisanie top keys\n",
    "    print(f\"Top-{top_k} attention key indices for sample {idx}: {top_keys}\")\n",
    "\n",
    "set_seed(234325352)\n",
    "random.seed(234325352)\n",
    "\n",
    "# Losowe 10 przykładów z automatycznym wykrywaniem kluczowych key_indexów\n",
    "for idx in random.sample(range(len(evaluate_set)), 20):\n",
    "    visualize_attention_auto_keys(detector_model, evaluate_set, idx, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86022f02",
   "metadata": {
    "id": "6FbNQOoWXEqo"
   },
   "source": [
    "# Analiza Macierzy Uwagi w Modelu OddShapeDetector\n",
    "\n",
    "W tej analizie przyjrzymy się macierzom uwagi w wytrenowanym modelu **OddShapeDetector**, który został zaprojektowany do wykrywania jednego kształtu innego od pozostałych kształtów na obrazach o rozmiarze 64x64 piksele. Model wykorzystuje mechanizm **self-attention**, aby skupić się na istotnych obszarach obrazu, co pozwala precyzyjnie przewidzieć położenie nietypowego kształtu.\n",
    "\n",
    "Wyniki, w tym obrazy wejściowe z **Ground Truth (GT)**, predykcjami i punktami o najwyższej uwadze, a także **heatmapy** w skali liniowej i logarytmicznej, zostały przedstawione powyżej dla różnych obrazów ze zbioru testowego.\n",
    "\n",
    "---\n",
    "\n",
    "## Gdzie Model Skupia Swoją Uwagę?\n",
    "\n",
    "Analiza wizualizacji różnych próbek testowych modelu **OddShapeDetector** ujawnia, że jego uwaga koncentruje się głównie na obszarach obrazu istotnych dla lokalizacji nietypowego kształtu, choć nie zawsze skupia się bezpośrednio na odstającym kształcie. Na przykład w próbce o numerze **9013**, gdzie błąd **RMSE** wynosi zaledwie **1.02**, uwaga modelu jest wyraźnie zlokalizowana w pobliżu **Ground Truth (GT)**. Obraz wejściowy pokazuje, że trzy punkty o najwyższej uwadze, zaznaczone niebieskimi kółkami, znajdują się blisko zielonej kropki GT, a mapa cieplna w skali liniowej podkreśla jasny, skoncentrowany obszar w tym regionie. Podobnie w próbce **15121**, z jeszcze niższym RMSE równym **0.83**, uwaga jest wyraźnie skupiona wokół GT, co pozwala modelowi osiągnąć wyjątkową precyzję predykcji.\n",
    "\n",
    "Jednak nie zawsze uwaga jest tak dobrze ukierunkowana. W próbce **18684**, gdzie RMSE wzrasta do **5.19**, uwaga rozkłada się dużo szerzej oraz nie wykazuje większego skupienia wokół GT, punkty o największej uwadze znajdują się wręcz w drugiej połowie obrazka niż odstający kształt. Wskazuje to, że model uwzględnia szerszy kontekst przestrzenny – niekoniecznie korzystny dla dokładnej lokalizacji. Podobne zjawisko obserwujemy w próbce **19159** z RMSE **2.96**, gdzie uwaga jest rozrzucona w 3 różne rogi obrazka, przy czym jeden z punktów wskazujących największą uwagę sieci znajduje się blisko odstającego kształtu, ale pozostałe dwa są od niego skrajnie odległe.\n",
    "\n",
    "Warto zauważyć, że sieć praktycznie nigdy nie skupia się bezpośrednio wewnątrz pewnego kształtu. Zamiast tego skupia się ona na analizowaniu brzegów, co sugeruje, że sieć szuka najpierw miejsc, gdzie brzegi kształtów tworzą nieregularne wzory i na podstawie tych miejsc stara się ustalić środek odstającego kształtu. Tłumaczy to dlaczego sieć w próbce **18684** najbardziej skupiła się na badaniu miejsc, gdzie nachodzą na siebie koła, a nie wokół faktycznego odstającego kształtu, który w tym wypadku jest praktycznie całkowicie zakryty przez inne koło. Analogicznie sieć bada nietypowe brzegi w większości próbek, zwłaszcza w próbkach **23173**, **3555**, **4344**.\n",
    "\n",
    "Nie wyklucza to jednak sytuacji, gdzie sieć czasami skupia uwagę w miejscach, które nie są stykiem dwóch nachodzących na siebie kształtów ani nie są okolicą brzegu odstającego kształtu. Jednak jak widać z wyników RMSE oraz wykresów w skali logarytmicznej to, że największą uwagę model kieruje nie zawsze w prawidłowe miejsce, nie blokuje możliwości dość dobrej predykcji pozycji szukanego kształtu. Praktycznie zawsze model trafia w okolice środka odstającego kształtu lub w najgorszej sytuacji przynajmniej w okolice jego brzegu.\n",
    "\n",
    "---\n",
    "\n",
    "## Spójne Zachowania w Próbkach\n",
    "\n",
    "Analiza zachowania modelu **OddShapeDetector** na podstawie różnych próbek testowych ujawnia pewne powtarzające się wzorce w sposobie, w jaki model kieruje swoją uwagę, co zostało częściowo przedstawione w sekcji _\"Gdzie Model Skupia Swoją Uwagę?\"_. Model konsekwentnie koncentruje się na obszarach obrazu istotnych dla lokalizacji nietypowego kształtu, choć rzadko skupia się bezpośrednio na samym odstającym elemencie. Zamiast tego, uwaga często kierowana jest na **brzegi kształtów oraz miejsca ich styku**, co wskazuje, że model poszukuje nieregularności i granic, aby ustalić położenie anomalii.\n",
    "\n",
    "W wielu przypadkach, takich jak próbki **9013 (RMSE: 1.02)** czy **15121 (RMSE: 0.83)**, uwaga modelu jest precyzyjnie zlokalizowana w pobliżu odstającego kształtu. W tych przykładach mapa cieplna wyraźnie podkreśla skoncentrowane obszary wokół GT, a punkty o najwyższej uwadze znajdują się blisko zielonej kropki, co pozwala modelowi osiągać wysoką dokładność predykcji. Te sytuacje pokazują, że gdy model prawidłowo identyfikuje kluczowe obszary, jego skuteczność jest znacznie wzrasta.\n",
    "\n",
    "Z kolei w bardziej złożonych scenariuszach, takich jak próbka **18684 (RMSE: 5.19)**, uwaga modelu rozprasza się na szerszym obszarze i nie koncentruje się wokół GT. Punkty o największej uwadze znajdują się daleko od odstającego kształtu, czasem w zupełnie innej części obrazu, co sugeruje, że model uwzględnia szerszy kontekst przestrzenny. Podobnie w próbce **19159 (RMSE: 2.96)** uwaga rozkłada się na różne rogi obrazu, z jedynie jednym punktem blisko celu, podczas gdy pozostałe są znacząco oddalone. Te przypadki ilustrują, że model, mimo rozproszonej uwagi, nadal stara się wyciągać wnioski na podstawie dostępnych informacji.\n",
    "\n",
    "Charakterystycznym i spójnym zachowaniem modelu jest jego tendencja do **analizowania brzegów kształtów zamiast ich wnętrz**. Na przykład w próbce **18684** uwaga skupia się na miejscach, gdzie koła nachodzą na siebie, a nie na samym odstającym kształcie, który jest częściowo zasłonięty. Analogiczne zachowanie obserwujemy w próbkach takich jak **23173**, **3555** czy **4344**, gdzie model bada nieregularne granice i styki kształtów, traktując je jako wskazówki do lokalizacji anomalii. To podejście wydaje się być kluczową strategią modelu w wykrywaniu nietypowych elementów.\n",
    "\n",
    "Co istotne, nawet gdy uwaga nie jest skierowana bezpośrednio na odstający kształt, model często osiąga zadowalające wyniki predykcji. W próbce **7057 (RMSE: 2.05)**, mimo rozproszonej uwagi, błąd pozostaje stosunkowo niski, co wskazuje na zdolność modelu do integrowania informacji z różnych obszarów obrazu. Podobnie w próbkach takich jak **19159** czy **18684**, choć uwaga jest rozłożona nierównomiernie, predykcje trafiają w okolice środka lub brzegu odstającego kształtu. Sugeruje to, że model wykorzystuje **kontekstowe dane z otoczenia**, co pozwala mu na elastyczność w trudniejszych przypadkach.\n",
    "\n",
    "---\n",
    "\n",
    "## Korelacja Między Uwagą a Położeniem Nietypowego Kształtu\n",
    "\n",
    "Analiza wyników pozwala na pogłębione spojrzenie na korelację między rozkładem uwagi modelu a położeniem nietypowego kształtu.\n",
    "\n",
    "W próbkach o **niskim RMSE**, takich jak:\n",
    "\n",
    "- **23177 (RMSE: 1.22)**\n",
    "- **9013 (RMSE: 1.02)**\n",
    "- **15121 (RMSE: 0.83)**\n",
    "\n",
    "obserwujemy **silną korelację** między obszarami o wysokiej uwadze a położeniem szukanego kształtu (Ground Truth, GT). W tych przypadkach punkty o najwyższej uwadze są zlokalizowane w bezpośrednim sąsiedztwie GT, a **heatmapy** (szczególnie w skali liniowej) pokazują wyraźne skupienie uwagi wokół odstającego kształtu. Sugeruje to, że **mechanizm self-attention** skutecznie identyfikuje kluczowe cechy obrazu, takie jak granice nietypowego kształtu, co prowadzi do precyzyjnych predykcji.\n",
    "\n",
    "W próbkach o **wyższym RMSE**, takich jak:\n",
    "\n",
    "- **13368 (RMSE: 3.54)**\n",
    "- **18684 (RMSE: 5.19)**\n",
    "\n",
    "korelacja między uwagą a położeniem GT jest **znacznie słabsza**. Uwaga modelu jest rozproszona, a punkty o najwyższej wadze znajdują się w obszarach **odległych od GT**, co wskazuje na trudności w identyfikacji właściwego kontekstu przestrzennego.\n",
    "\n",
    "Na przykład w próbce **18684** model skupia się na stykach kształtów, które **nie są bezpośrednio związane** z odstającym elementem, co prowadzi do większego błędu predykcji.\n",
    "\n",
    "Jednak nawet w tych przypadkach model często trafia w **okolice brzegu nietypowego kształtu**, co sugeruje, że korelacja między uwagą a GT istnieje, ale jest **zaburzona przez złożoność obrazu**, taką jak nakładanie się kształtów.\n",
    "\n",
    "---\n",
    "\n",
    "## Zaskakujące lub Niejasne Wzorce\n",
    "\n",
    "Ciekawym przypadkiem są próbki, takie jak:\n",
    "\n",
    "- **830 (RMSE: 9.22)**\n",
    "- **24375 (RMSE: 0.48)**\n",
    "- **4344 (RMSE: 4.04)**\n",
    "\n",
    "W tych próbkach korelacja między uwagą a GT jest zaburzona. Próbki **830** oraz **4344** mają wysokie wyniki RMSE, pomimo tego, że uwaga sieci jest skupiona wokół odstającego kształtu. W przypadku próbki **830** odstający kształt prawie całkowicie zakryty jest przez inny kształt, co powoduje, że sieć skupiona na analizie brzegu, a nie wnętrza kształtu, ma problem z ustaleniem prawdopodobnego środka odstającego kształtu. Przypadek próbki **4344** jest prostszy, jednak mimo to sieć uzyskała predykcję znacznie odbiegającą od środka odstającego kształtu. Ponownie prawdopodobną przyczyną był brak analizy wnętrza kształtu, a jedynie analiza nieregularnego brzegu, który w tym wypadku był bardzo skomplikowany (szukany trójkąt styka się z dwoma kołami).\n",
    "\n",
    "W przypadku próbki **24375** mamy do czynienia z odwrotną sytuacją. Pomimo tego, że sieć najbardziej skupiła się na punktach odległych od Ground Truth, to uzyskała jeden z najlepszych wyników RMSE. Warto zwrócić w tym przypadku uwagę na heatmapę w skali logarytmicznej. Widać z niej, że choć model najbardziej skupił się w odległych od prawdy punktach, to wykazuje też spore skupienie w całej okolicy odstającego kształtu. Oznacza to, że model potrafi również wyciągnąć ważne informacje z otoczenia, które nie jest dla niego głównym źródłem uwagi, i zastosować te informacje do bardzo dokładnej predykcji.\n",
    "\n",
    "------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-base-py",
   "language": "python",
   "display_name": "Python [conda env:base] *"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
